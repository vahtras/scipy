<!DOCTYPE html>
<html>
  <head>
    <title></title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>

    <style type="text/css">
      @import url(http://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(http://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body {
        font-family: 'Droid Serif';
      }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        -moz-border-radius: 5px;
        -web-border-radius: 5px;
        background: #e7e8e2;
        border-radius: 5px;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #777872;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 0.8em;
      }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 1em;
      }
    </style>
    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

  </head>
  <body>
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
    <script src="/scipy/refreeze/js/springy.js"></script>
    <script src="/scipy/refreeze/js/springyui.js"></script>
    <script>
        
    </script>
    <textarea id="source">
name: inverse
layout: true
class: center, middle, inverse

---

# Introduction to scipy

## Olav Vahtras

Computational Python

---

layout: false

## SciPy

* Collection of scientific libraries in different disciplines
* http://www.scipy.org
* http://www.scipy-lectures.org

---

### Optimization

* minimizing
* curve fitting
* least-square

### Integration
---

### Minimizing

&lt;img src=&#34;morse.png&#34; height=&#34;300&#34; /&gt;

```
    import math
    def f(r):
       d=2.5; a=1.5; r0=0.5
       e=math.exp(-a*(r-r0))
       return d*(1-e)**2
```

---

### The Brent method

* Bracketing approach
* No derivatives


    &gt;&gt;&gt; minx = optimize.brent(f)
    &gt;&gt;&gt; print minx, f(minx)
    0.500000001794 1.81016124365e-17

---

### Conjugent gradient


* Gradient function can be given
* Approximate gradient used if not


```
    &gt;&gt;&gt; minx = optimize.fmin_cg(f, [1])
    Optimization terminated successfully.
             Current function value: 0.000000
             Iterations: 3
             Function evaluations: 21
             Gradient evaluations: 7
    &gt;&gt;&gt; print minx, f(minx)
    [ 0.50000005] 1.39392421709e-14
```
--

### Supply gradient

```
    &gt;&gt;&gt; def g(r):
    ...   d=2.5; a=1.5; r0=0.5
    ...   e=math.exp(-a*(r-r0))
    &gt;&gt;&gt; minx = optimize.fmin_cg(f, [1], fprime=g)
    &gt;&gt;&gt; print minx, f(minx)
    Optimization terminated successfully.
             Current function value: 0.000000
             Iterations: 3
             Function evaluations: 7
             Gradient evaluations: 7
    [ 0.50000005] 1.68621264068e-14
```
    
---


### The Newton method

* Based on a local second-order expansion
$$
    f(x) = f(x_0) + (x-x_0)f&#39;(x_0) + \frac 1 2 (x-x_0)f&#39;&#39;(x_0)
$$

* Minimized by
$$
    f&#39;(x) = f&#39;(x_0) + (x-x_0)f&#39;&#39;(x_0) = 0
$$

* Iterative method
$$
    x = x_0 - f&#39;(x_0)/f&#39;&#39;(x_0)
$$

---

### using the Newton method

* Hessian is optional - requires more gradient evaluations
```
    &gt;&gt;&gt; minx = optimize.fmin_ncg(f, [1], fprime=g)
    &gt;&gt;&gt; print minx, f(minx)
    Optimization terminated successfully.
             Current function value: 0.000000
             Iterations: 6
             Function evaluations: 11
             Gradient evaluations: 18
             Hessian evaluations: 0
    [ 0.5] 3.53519126782e-19
```    

---

### supply the Hessian

```
    &gt;&gt;&gt; def h(r):
    &gt;&gt;&gt;   d=2.5; a=1.5; r0=0.5
    &gt;&gt;&gt;   e=math.exp(-a*(r-r0))
    &gt;&gt;&gt;   return 2*a*d*(-a*e*(1-e) + e*(a*e))
    Optimization terminated successfully.
             Current function value: 0.000000
             Iterations: 6
             Function evaluations: 11
             Gradient evaluations: 6
             Hessian evaluations: 6
    [ 0.5] 1.49714017434e-23
```

---

### using quasi-Newton method

```
* some approximation of the Hessian is maintained
* when the Hessian is too expensive
```

```
    &gt;&gt;&gt; minx = optimize.fmin_bfgs(f, [1], fprime=g)
    &gt;&gt;&gt; print minx, f(minx)
    Optimization terminated successfully.
             Current function value: 0.000000
             Iterations: 3
             Function evaluations: 6
             Gradient evaluations: 6
    [ 0.49999989] 7.41660768733e-14
```

---

### using  lBFGS

* inprinciple as BFGS - does not store explicit Hessians


```
    &gt;&gt;&gt; minx, fminx, dic = optimize.fmin_l_bfgs_b(f, [1], fprime=g)
    &gt;&gt;&gt; print minx, f(minx)
    &gt;&gt;&gt; for k in dic:
    ...    print k, dic[k]
    [ 0.5] 2.17833574202e-17
    warnflag 0
    task CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL
    grad [ -2.21387792e-08]
    funcalls 9
```

---

### Curve-Fitting

* Consider the Morse example ``d=2.5, a=1.5 r0=0.5``
* Add numerical noise


```
    &gt;&gt;&gt; x = numpy.arange(0, 5, .1)    
    &gt;&gt;&gt; y = f(x) + 0.01*numpy.random.normal(len(x))
    &gt;&gt;&gt; print optimize.curve_fit(f, x, y)
    (array([ 3.79505629,  0.37332076, -1.94009589]), array([[ 0.66594742, -0.15499145, -0.71700161],
           [-0.15499145,  0.03847578,  0.18918748],
           [-0.71700161,  0.18918748,  1.00385781]]))

```

---

* Lower the noise


```
    &gt;&gt;&gt; x = numpy.arange(0, 5, .1)    
    &gt;&gt;&gt; y = f(x) + 0.001*numpy.random.normal(len(x))
    &gt;&gt;&gt; print optimize.curve_fit(f, x, y)
    (array([ 2.5468709 ,  1.51754354,  0.49529173]), array([[  1.66450955e-05,  -1.22341586e-05,   2.42381535e-06],
           [ -1.22341586e-05,   2.43545494e-05,  -5.59536473e-06],
           [  2.42381535e-06,  -5.59536473e-06,   1.89802521e-06]]))
```
---

### Least-Square methods


#### Example

Simple least square fitting to a straight line

* Initialize arrays

```
    &gt;&gt;&gt; import numpy
    &gt;&gt;&gt; from scipy.optimize import leastsq
    &gt;&gt;&gt; x = numpy.arange(0, 10, .01)
    &gt;&gt;&gt; k0 = 3.0
    &gt;&gt;&gt; l0 = 1.0
    &gt;&gt;&gt; y = k0 * x + l0 + numpy.random.randn(len(x))
```

---

* Define residual function
* The parameters to be optimize for the first tuple

```
    &gt;&gt;&gt; def resid(p, y, x):
    ...    k,l = p
    ...    return y - k*x - l
```

---

* The arguments to ``leastsq`` are the residual function, the initial parameters ``p0`` and the data set ``(y, x)`` in ``args`` 


```
    &gt;&gt;&gt; p0 = numpy.array([k0, l0])
    &gt;&gt;&gt; plsq = leastsq(resid, p0, args=(y, x))
    &gt;&gt;&gt; k, l = plsq[0]
    &gt;&gt;&gt; print k, l
    k=3.008980 l=0.964311
```
---

### Integration

* uses package `scipy.integrate`
* definite integrals in 1-3 dimensions

---

#### Example


The integral

$$   \int_{-\infty}^\infty e^{-x^2} = \sqrt{\pi} $$


```
    &gt;&gt;&gt; from scipy.integrate import quad
    &gt;&gt;&gt; sqrtpi = math.sqrt(math.pi)
    &gt;&gt;&gt; def e2(x):
    ...    return math.exp(-x*x)
    &gt;&gt;&gt; res, err = quad(e2, -10, 10)
    &gt;&gt;&gt; print res, sqrtpi, abs(res-sqrtpi)
    1.77245385091 1.77245385091 2.22044604925e-16
```

---

Other module in scipy

* Machine learning `scikit-learn`
* Image processing `scikit-image`
* Statistics
* Symbolic mathematics `sympy`
* 3D-plotting 
    </textarea>

    <script src="/scipy/refreeze/js/remark-latest.min.js" type="text/javascript"></script>
    <script type="text/javascript">
      var hljs = remark.highlighter.engine;
    </script>
    <script src="/scipy/refreeze/js/high.js" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create({
          highlightStyle: 'sunburst',
          highlightLanguage: 'remark'
        }) ;
    </script>
  </body>
</html>